{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet /kaggle/input/kerasapplications\n!pip install --quiet /kaggle/input/efficientnet-keras-source-code","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math, os, random, re, gc\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nimport efficientnet.tfkeras as efn\nfrom functools import partial\nfrom sklearn.model_selection import KFold","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nstrategy.num_replicas_in_sync","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"8"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-tfrecords-512x512')\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nAUG_BATCH = BATCH_SIZE\nIMAGE_SIZE = [512, 512]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 14\nSEED = 42\nFOLDS = 5","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear    = math.pi * shear    / 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n                               zero,            one/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform_mat(image, DIM=IMAGE_SIZE[0]):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 \n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32')\n    \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d, [DIM, DIM,3])\n\ndef dropout(image, DIM=IMAGE_SIZE[0], PROBABILITY = 0.5, CT = 4, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3]) \n        three = image[ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n    image = tf.reshape(image,[DIM,DIM,3])\n    return image","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot(image,label):\n    return image,tf.one_hot(label,len(CLASSES))\n\n\ndef cutmix(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2\n\ndef mixup(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        # MAKE CUTMIX LABEL\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n        \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2\n\ndef transform(image,label):\n    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    SWITCH = 0.5\n    CUTMIX_PROB = 0.666\n    MIXUP_PROB = 0.666\n    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n    image1 = []\n    for j in range(AUG_BATCH):\n        img = transform_mat(image[j,])\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n        image1.append(img)\n        \n    image1 = tf.reshape(tf.stack(image1),(AUG_BATCH,DIM,DIM,3))\n    image2, label2 = cutmix(image1, label, CUTMIX_PROB)\n    image3, label3 = mixup(image1, label, MIXUP_PROB)\n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n        labs.append(P*label2[j,]+(1-P)*label3[j,])\n    \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image4,label4","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0     ###\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, 0.3)\n    #image = dropout(image)\n    return image, label\n\ndef get_training_dataset(FILENAMES):\n    dataset = load_dataset(FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048, seed = SEED)\n    dataset = dataset.batch(AUG_BATCH)\n    dataset = dataset.map(onehot, num_parallel_calls=AUTOTUNE)\n    #dataset = dataset.map(transform, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef get_validation_dataset(FILENAMES):\n    dataset = load_dataset(FILENAMES, labeled=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(onehot, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr_callback(batch_size=BATCH_SIZE):\n    lr_start   = 0.0000001\n    lr_max     = 0.000000250 * strategy.num_replicas_in_sync * batch_size\n    lr_min     = 0.0000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\n    return lr_callback\n\nget_lr_callback()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<tensorflow.python.keras.callbacks.LearningRateScheduler at 0x7fb779a10410>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    with strategy.scope():       \n        input_layer = tf.keras.layers.Input(shape=(512,512,3))\n        base = efn.EfficientNetB4(input_shape=(512,512,3),weights='noisy-student',include_top=False)\n        base.trainable = True\n        \n        for layer in base.layers:\n            if isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable =  False\n        \n        x = base(input_layer)\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        x = tf.keras.layers.Dense(128)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        output_layer = tf.keras.layers.Dense(5,activation='softmax')(x)\n        model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n        \n        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n        loss = tf.keras.losses.CategoricalCrossentropy(\n            from_logits=False,\n            label_smoothing=0.1,  #0.001\n            name='categorical_crossentropy')\n        \n        model.compile(optimizer=opt,loss=loss,metrics=['categorical_accuracy'])\n    return model\n\nmodel = get_model()\nprint(model.summary())\ndel model","execution_count":10,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_noisy-student_notop.h5\n71680000/71678424 [==============================] - 1s 0us/step\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 512, 512, 3)]     0         \n_________________________________________________________________\nefficientnet-b4 (Functional) (None, 16, 16, 1792)      17673816  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1792)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1792)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               229504    \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 128)               512       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 645       \n=================================================================\nTotal params: 17,904,477\nTrainable params: 17,653,821\nNon-trainable params: 250,656\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/Id_train*.tfrec\")\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f, (trn_ind, val_ind) in enumerate(skf.split(TRAINING_FILENAMES)):\n    print(\"FOLD-{}\".format(f+1))\n    print()\n    \n    TRAIN_FILENAMES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES'])\n    VALID_FILENAMES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES'])\n\n    train_dataset = get_training_dataset(TRAIN_FILENAMES)\n    valid_dataset = get_validation_dataset(VALID_FILENAMES)\n    \n    NUM_TRAINING_IMAGES = count_data_items(TRAIN_FILENAMES)\n    NUM_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n    \n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n    VALID_STEPS = NUM_VALID_IMAGES // BATCH_SIZE\n    \n    CP = tf.keras.callbacks.ModelCheckpoint(\"fkfold_model-{}.h5\".format(f+1),\n                                            monitor='val_categorical_accuracy', verbose=0, save_best_only=True,\n                                            save_weights_only=False, mode='max', save_freq='epoch')\n    \n    model = get_model()\n    history = model.fit(train_dataset, \n                        steps_per_epoch=STEPS_PER_EPOCH, \n                        epochs=EPOCHS,\n                        validation_data=valid_dataset,\n                        validation_steps=VALID_STEPS,\n                        callbacks = [get_lr_callback(BATCH_SIZE), CP])\n    \n    del model; z = gc.collect()","execution_count":12,"outputs":[{"output_type":"stream","text":"FOLD-1\n\nEpoch 1/14\n267/267 [==============================] - 150s 279ms/step - loss: 2.2999 - categorical_accuracy: 0.2189 - val_loss: 1.6766 - val_categorical_accuracy: 0.2327\nEpoch 2/14\n267/267 [==============================] - 60s 223ms/step - loss: 1.7515 - categorical_accuracy: 0.4346 - val_loss: 0.9043 - val_categorical_accuracy: 0.8248\nEpoch 3/14\n267/267 [==============================] - 59s 223ms/step - loss: 1.2119 - categorical_accuracy: 0.6872 - val_loss: 0.7559 - val_categorical_accuracy: 0.8627\nEpoch 4/14\n267/267 [==============================] - 59s 222ms/step - loss: 1.0389 - categorical_accuracy: 0.7593 - val_loss: 0.7409 - val_categorical_accuracy: 0.8771\nEpoch 5/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.9618 - categorical_accuracy: 0.7878 - val_loss: 0.7101 - val_categorical_accuracy: 0.8738\nEpoch 6/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.8922 - categorical_accuracy: 0.8149 - val_loss: 0.6844 - val_categorical_accuracy: 0.8781\nEpoch 7/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.8294 - categorical_accuracy: 0.8435 - val_loss: 0.6566 - val_categorical_accuracy: 0.8944\nEpoch 8/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.7906 - categorical_accuracy: 0.8552 - val_loss: 0.6663 - val_categorical_accuracy: 0.8826\nEpoch 9/14\n267/267 [==============================] - 59s 223ms/step - loss: 0.7741 - categorical_accuracy: 0.8637 - val_loss: 0.6596 - val_categorical_accuracy: 0.8916\nEpoch 10/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.7390 - categorical_accuracy: 0.8752 - val_loss: 0.6491 - val_categorical_accuracy: 0.8849\nEpoch 11/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.7152 - categorical_accuracy: 0.8863 - val_loss: 0.6492 - val_categorical_accuracy: 0.8904\nEpoch 12/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.6956 - categorical_accuracy: 0.8941 - val_loss: 0.6452 - val_categorical_accuracy: 0.8899\nEpoch 13/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.6981 - categorical_accuracy: 0.8922 - val_loss: 0.6437 - val_categorical_accuracy: 0.8918\nEpoch 14/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.6750 - categorical_accuracy: 0.9064 - val_loss: 0.6398 - val_categorical_accuracy: 0.8909\nFOLD-2\n\nEpoch 1/14\n267/267 [==============================] - 141s 268ms/step - loss: 2.4908 - categorical_accuracy: 0.1960 - val_loss: 1.6971 - val_categorical_accuracy: 0.1972\nEpoch 2/14\n267/267 [==============================] - 59s 221ms/step - loss: 1.7783 - categorical_accuracy: 0.4356 - val_loss: 0.9073 - val_categorical_accuracy: 0.8125\nEpoch 3/14\n267/267 [==============================] - 59s 221ms/step - loss: 1.2178 - categorical_accuracy: 0.6850 - val_loss: 0.7843 - val_categorical_accuracy: 0.8617\nEpoch 4/14\n267/267 [==============================] - 59s 221ms/step - loss: 1.0625 - categorical_accuracy: 0.7471 - val_loss: 0.7512 - val_categorical_accuracy: 0.8707\nEpoch 5/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.9757 - categorical_accuracy: 0.7907 - val_loss: 0.7262 - val_categorical_accuracy: 0.8705\nEpoch 6/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.8949 - categorical_accuracy: 0.8172 - val_loss: 0.7478 - val_categorical_accuracy: 0.8546\nEpoch 7/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.8606 - categorical_accuracy: 0.8313 - val_loss: 0.6964 - val_categorical_accuracy: 0.8769\nEpoch 8/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.8077 - categorical_accuracy: 0.8465 - val_loss: 0.7150 - val_categorical_accuracy: 0.8724\nEpoch 9/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.7910 - categorical_accuracy: 0.8526 - val_loss: 0.6917 - val_categorical_accuracy: 0.8828\nEpoch 10/14\n267/267 [==============================] - 59s 223ms/step - loss: 0.7589 - categorical_accuracy: 0.8637 - val_loss: 0.6693 - val_categorical_accuracy: 0.8911\nEpoch 11/14\n267/267 [==============================] - 59s 222ms/step - loss: 0.7225 - categorical_accuracy: 0.8821 - val_loss: 0.6677 - val_categorical_accuracy: 0.8840\nEpoch 12/14\n267/267 [==============================] - 59s 223ms/step - loss: 0.7251 - categorical_accuracy: 0.8838 - val_loss: 0.6801 - val_categorical_accuracy: 0.8864\nEpoch 13/14\n267/267 [==============================] - 60s 223ms/step - loss: 0.6985 - categorical_accuracy: 0.8927 - val_loss: 0.6736 - val_categorical_accuracy: 0.8875\nEpoch 14/14\n267/267 [==============================] - 59s 223ms/step - loss: 0.7045 - categorical_accuracy: 0.8895 - val_loss: 0.6721 - val_categorical_accuracy: 0.8906\nFOLD-3\n\nEpoch 1/14\n267/267 [==============================] - 140s 268ms/step - loss: 2.3576 - categorical_accuracy: 0.2096 - val_loss: 1.5255 - val_categorical_accuracy: 0.3317\nEpoch 2/14\n267/267 [==============================] - 59s 222ms/step - loss: 1.7384 - categorical_accuracy: 0.4404 - val_loss: 0.8558 - val_categorical_accuracy: 0.8198\nEpoch 3/14\n267/267 [==============================] - 60s 224ms/step - loss: 1.1939 - categorical_accuracy: 0.6935 - val_loss: 0.7781 - val_categorical_accuracy: 0.8606\nEpoch 4/14\n267/267 [==============================] - 60s 223ms/step - loss: 1.0528 - categorical_accuracy: 0.7536 - val_loss: 0.7878 - val_categorical_accuracy: 0.8537\nEpoch 5/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.9485 - categorical_accuracy: 0.7988 - val_loss: 0.7229 - val_categorical_accuracy: 0.8762\nEpoch 6/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.9018 - categorical_accuracy: 0.8143 - val_loss: 0.7317 - val_categorical_accuracy: 0.8719\nEpoch 7/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.8449 - categorical_accuracy: 0.8396 - val_loss: 0.7014 - val_categorical_accuracy: 0.8757\nEpoch 8/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.8007 - categorical_accuracy: 0.8574 - val_loss: 0.7175 - val_categorical_accuracy: 0.8681\nEpoch 9/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.7675 - categorical_accuracy: 0.8640 - val_loss: 0.6843 - val_categorical_accuracy: 0.8868\nEpoch 10/14\n267/267 [==============================] - 61s 227ms/step - loss: 0.7481 - categorical_accuracy: 0.8717 - val_loss: 0.6827 - val_categorical_accuracy: 0.8816\nEpoch 11/14\n267/267 [==============================] - 61s 227ms/step - loss: 0.7271 - categorical_accuracy: 0.8793 - val_loss: 0.6967 - val_categorical_accuracy: 0.8823\nEpoch 12/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.7174 - categorical_accuracy: 0.8896 - val_loss: 0.6722 - val_categorical_accuracy: 0.8859\nEpoch 13/14\n267/267 [==============================] - 61s 228ms/step - loss: 0.6990 - categorical_accuracy: 0.8923 - val_loss: 0.6806 - val_categorical_accuracy: 0.8793\nEpoch 14/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.6885 - categorical_accuracy: 0.8974 - val_loss: 0.6828 - val_categorical_accuracy: 0.8873\nFOLD-4\n\nEpoch 1/14\n267/267 [==============================] - 143s 270ms/step - loss: 2.4417 - categorical_accuracy: 0.2097 - val_loss: 1.6814 - val_categorical_accuracy: 0.2228\nEpoch 2/14\n267/267 [==============================] - 60s 226ms/step - loss: 1.7728 - categorical_accuracy: 0.4442 - val_loss: 0.8950 - val_categorical_accuracy: 0.8165\nEpoch 3/14\n267/267 [==============================] - 60s 223ms/step - loss: 1.1951 - categorical_accuracy: 0.6772 - val_loss: 0.7654 - val_categorical_accuracy: 0.8613\nEpoch 4/14\n267/267 [==============================] - 61s 227ms/step - loss: 1.0551 - categorical_accuracy: 0.7529 - val_loss: 0.7570 - val_categorical_accuracy: 0.8660\nEpoch 5/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.9646 - categorical_accuracy: 0.7801 - val_loss: 0.7106 - val_categorical_accuracy: 0.8802\nEpoch 6/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.8900 - categorical_accuracy: 0.8175 - val_loss: 0.6975 - val_categorical_accuracy: 0.8776\nEpoch 7/14\n","name":"stdout"},{"output_type":"stream","text":"267/267 [==============================] - 60s 225ms/step - loss: 0.8502 - categorical_accuracy: 0.8332 - val_loss: 0.6767 - val_categorical_accuracy: 0.8826\nEpoch 8/14\n267/267 [==============================] - 61s 227ms/step - loss: 0.7969 - categorical_accuracy: 0.8546 - val_loss: 0.6513 - val_categorical_accuracy: 0.8970\nEpoch 9/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.7765 - categorical_accuracy: 0.8613 - val_loss: 0.6517 - val_categorical_accuracy: 0.8878\nEpoch 10/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.7463 - categorical_accuracy: 0.8751 - val_loss: 0.6449 - val_categorical_accuracy: 0.8932\nEpoch 11/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.7321 - categorical_accuracy: 0.8818 - val_loss: 0.6531 - val_categorical_accuracy: 0.8942\nEpoch 12/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.7120 - categorical_accuracy: 0.8872 - val_loss: 0.6377 - val_categorical_accuracy: 0.8970\nEpoch 13/14\n267/267 [==============================] - 61s 227ms/step - loss: 0.6900 - categorical_accuracy: 0.8982 - val_loss: 0.6512 - val_categorical_accuracy: 0.8956\nEpoch 14/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.6864 - categorical_accuracy: 0.8955 - val_loss: 0.6517 - val_categorical_accuracy: 0.8973\nFOLD-5\n\nEpoch 1/14\n267/267 [==============================] - 145s 274ms/step - loss: 2.3872 - categorical_accuracy: 0.2251 - val_loss: 1.5728 - val_categorical_accuracy: 0.3506\nEpoch 2/14\n267/267 [==============================] - 60s 225ms/step - loss: 1.7247 - categorical_accuracy: 0.4558 - val_loss: 0.8822 - val_categorical_accuracy: 0.8158\nEpoch 3/14\n267/267 [==============================] - 60s 224ms/step - loss: 1.1698 - categorical_accuracy: 0.6981 - val_loss: 0.7887 - val_categorical_accuracy: 0.8568\nEpoch 4/14\n267/267 [==============================] - 61s 227ms/step - loss: 1.0375 - categorical_accuracy: 0.7642 - val_loss: 0.7987 - val_categorical_accuracy: 0.8648\nEpoch 5/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.9417 - categorical_accuracy: 0.8059 - val_loss: 0.7618 - val_categorical_accuracy: 0.8672\nEpoch 6/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.9021 - categorical_accuracy: 0.8191 - val_loss: 0.7450 - val_categorical_accuracy: 0.8710\nEpoch 7/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.8473 - categorical_accuracy: 0.8344 - val_loss: 0.7101 - val_categorical_accuracy: 0.8686\nEpoch 8/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.7896 - categorical_accuracy: 0.8583 - val_loss: 0.6796 - val_categorical_accuracy: 0.8838\nEpoch 9/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.7549 - categorical_accuracy: 0.8703 - val_loss: 0.6769 - val_categorical_accuracy: 0.8802\nEpoch 10/14\n267/267 [==============================] - 61s 227ms/step - loss: 0.7447 - categorical_accuracy: 0.8749 - val_loss: 0.6726 - val_categorical_accuracy: 0.8802\nEpoch 11/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.7168 - categorical_accuracy: 0.8862 - val_loss: 0.6724 - val_categorical_accuracy: 0.8866\nEpoch 12/14\n267/267 [==============================] - 60s 224ms/step - loss: 0.7069 - categorical_accuracy: 0.8913 - val_loss: 0.6779 - val_categorical_accuracy: 0.8828\nEpoch 13/14\n267/267 [==============================] - 60s 225ms/step - loss: 0.6953 - categorical_accuracy: 0.8961 - val_loss: 0.6763 - val_categorical_accuracy: 0.8845\nEpoch 14/14\n267/267 [==============================] - 60s 226ms/step - loss: 0.6771 - categorical_accuracy: 0.9029 - val_loss: 0.6605 - val_categorical_accuracy: 0.8880\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}