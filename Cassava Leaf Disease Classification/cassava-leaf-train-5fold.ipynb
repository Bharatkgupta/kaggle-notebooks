{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet /kaggle/input/kerasapplications\n!pip install --quiet /kaggle/input/efficientnet-keras-source-code","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math, os, random, re, gc\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nimport efficientnet.tfkeras as efn\nfrom functools import partial\nfrom sklearn.model_selection import KFold","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\nstrategy.num_replicas_in_sync","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"8"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('cassava-leaf-disease-tfrecords-512x512')\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nAUG_BATCH = BATCH_SIZE\nIMAGE_SIZE = [512, 512]\nCLASSES = ['0', '1', '2', '3', '4']\nEPOCHS = 15\nSEED = 42\nFOLDS = 5","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear    = math.pi * shear    / 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n                               zero,            one/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform_mat(image, DIM=IMAGE_SIZE[0]):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 \n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32')\n    \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d, [DIM, DIM,3])\n\ndef dropout(image, DIM=IMAGE_SIZE[0], PROBABILITY = 0.5, CT = 4, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3]) \n        three = image[ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n    image = tf.reshape(image,[DIM,DIM,3])\n    return image","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot(image,label):\n    return image,tf.one_hot(label,len(CLASSES))\n\n\ndef cutmix(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2\n\ndef mixup(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        # MAKE CUTMIX LABEL\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n        \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2\n\ndef transform(image,label):\n    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n    DIM = IMAGE_SIZE[0]\n    CLASSES = 5\n    SWITCH = 0.5\n    CUTMIX_PROB = 0.666\n    MIXUP_PROB = 0.666\n    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n    image1 = []\n    for j in range(AUG_BATCH):\n        img = transform_mat(image[j,])\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n        image1.append(img)\n        \n    image1 = tf.reshape(tf.stack(image1),(AUG_BATCH,DIM,DIM,3))\n    image2, label2 = cutmix(image1, label, CUTMIX_PROB)\n    image3, label3 = mixup(image1, label, MIXUP_PROB)\n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n        labs.append(P*label2[j,]+(1-P)*label3[j,])\n    \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image4,label4","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0     ###\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, 0.3)\n    image = dropout(image)\n    return image, label\n\ndef get_training_dataset(FILENAMES):\n    dataset = load_dataset(FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048, seed = SEED)\n    dataset = dataset.batch(AUG_BATCH)\n    dataset = dataset.map(transform, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\ndef get_validation_dataset(FILENAMES):\n    dataset = load_dataset(FILENAMES, labeled=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(onehot, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr_callback(batch_size=BATCH_SIZE):\n    lr_start   = 0.000001\n    lr_max     = 0.00000250 * strategy.num_replicas_in_sync * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.85\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    \n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\n    return lr_callback","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    with strategy.scope():       \n        input_layer = tf.keras.layers.Input(shape=(512,512,3))\n        base = efn.EfficientNetB4(input_shape=(512,512,3),weights='noisy-student',include_top=False)\n        base.trainable = True\n        \n        x = base(input_layer)\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        output_layer = tf.keras.layers.Dense(5,activation='softmax')(x)\n        model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n        \n        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n        loss = tf.keras.losses.CategoricalCrossentropy(\n            from_logits=False,\n            label_smoothing=0.0001,  #0.001\n            name='categorical_crossentropy')\n        \n        model.compile(optimizer=opt,loss=loss,metrics=['categorical_accuracy'])\n    return model\n\nmodel = get_model()\nprint(model.summary())\ndel model","execution_count":10,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_noisy-student_notop.h5\n71680000/71678424 [==============================] - 1s 0us/step\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 512, 512, 3)]     0         \n_________________________________________________________________\nefficientnet-b4 (Functional) (None, 16, 16, 1792)      17673816  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 1792)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 1792)              0         \n_________________________________________________________________\ndense (Dense)                (None, 5)                 8965      \n=================================================================\nTotal params: 17,682,781\nTrainable params: 17,557,581\nNon-trainable params: 125,200\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/Id_train*.tfrec\")\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f, (trn_ind, val_ind) in enumerate(skf.split(TRAINING_FILENAMES)):\n    print(\"FOLD-{}\".format(f+1))\n    print()\n    \n    TRAIN_FILENAMES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[trn_ind]['TRAINING_FILENAMES'])\n    VALID_FILENAMES = list(pd.DataFrame({'TRAINING_FILENAMES': TRAINING_FILENAMES}).loc[val_ind]['TRAINING_FILENAMES'])\n\n    train_dataset = get_training_dataset(TRAIN_FILENAMES)\n    valid_dataset = get_validation_dataset(VALID_FILENAMES)\n    \n    NUM_TRAINING_IMAGES = count_data_items(TRAIN_FILENAMES)\n    NUM_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n    \n    STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n    VALID_STEPS = NUM_VALID_IMAGES // BATCH_SIZE\n    \n    model = get_model()\n    history = model.fit(train_dataset, \n                        steps_per_epoch=STEPS_PER_EPOCH, \n                        epochs=EPOCHS,\n                        validation_data=valid_dataset,\n                        validation_steps=VALID_STEPS,\n                        callbacks = [get_lr_callback(BATCH_SIZE)])\n    \n    model.save(\"fkfold_model-{}.h5\".format(f+1))\n    \n    del model; z = gc.collect()","execution_count":12,"outputs":[{"output_type":"stream","text":"FOLD-1\n\nEpoch 1/15\n267/267 [==============================] - 203s 408ms/step - loss: 1.5873 - categorical_accuracy: 0.2541 - val_loss: 1.5865 - val_categorical_accuracy: 0.2680\nEpoch 2/15\n267/267 [==============================] - 81s 305ms/step - loss: 0.9737 - categorical_accuracy: 0.6751 - val_loss: 0.4278 - val_categorical_accuracy: 0.8535\nEpoch 3/15\n267/267 [==============================] - 85s 320ms/step - loss: 0.7724 - categorical_accuracy: 0.7519 - val_loss: 0.4726 - val_categorical_accuracy: 0.8409\nEpoch 4/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.7610 - categorical_accuracy: 0.7573 - val_loss: 0.4179 - val_categorical_accuracy: 0.8568\nEpoch 5/15\n267/267 [==============================] - 89s 334ms/step - loss: 0.7299 - categorical_accuracy: 0.7795 - val_loss: 0.3989 - val_categorical_accuracy: 0.8712\nEpoch 6/15\n267/267 [==============================] - 83s 309ms/step - loss: 0.7098 - categorical_accuracy: 0.7768 - val_loss: 0.4926 - val_categorical_accuracy: 0.8362\nEpoch 7/15\n267/267 [==============================] - 84s 314ms/step - loss: 0.6994 - categorical_accuracy: 0.7840 - val_loss: 0.4915 - val_categorical_accuracy: 0.8232\nEpoch 8/15\n267/267 [==============================] - 84s 314ms/step - loss: 0.6604 - categorical_accuracy: 0.7929 - val_loss: 0.3759 - val_categorical_accuracy: 0.8762\nEpoch 9/15\n267/267 [==============================] - 80s 299ms/step - loss: 0.6511 - categorical_accuracy: 0.8020 - val_loss: 0.3641 - val_categorical_accuracy: 0.8767\nEpoch 10/15\n267/267 [==============================] - 82s 308ms/step - loss: 0.6238 - categorical_accuracy: 0.8127 - val_loss: 0.3425 - val_categorical_accuracy: 0.8904\nEpoch 11/15\n267/267 [==============================] - 82s 306ms/step - loss: 0.6147 - categorical_accuracy: 0.8166 - val_loss: 0.3247 - val_categorical_accuracy: 0.8902\nEpoch 12/15\n267/267 [==============================] - 81s 303ms/step - loss: 0.6002 - categorical_accuracy: 0.8178 - val_loss: 0.3496 - val_categorical_accuracy: 0.8802\nEpoch 13/15\n267/267 [==============================] - 84s 316ms/step - loss: 0.5857 - categorical_accuracy: 0.8257 - val_loss: 0.3433 - val_categorical_accuracy: 0.8845\nEpoch 14/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.5800 - categorical_accuracy: 0.8328 - val_loss: 0.3187 - val_categorical_accuracy: 0.8954\nEpoch 15/15\n267/267 [==============================] - 82s 308ms/step - loss: 0.5689 - categorical_accuracy: 0.8336 - val_loss: 0.3231 - val_categorical_accuracy: 0.8946\nFOLD-2\n\nEpoch 1/15\n267/267 [==============================] - 204s 403ms/step - loss: 1.6559 - categorical_accuracy: 0.2057 - val_loss: 1.5225 - val_categorical_accuracy: 0.4631\nEpoch 2/15\n267/267 [==============================] - 84s 316ms/step - loss: 1.0424 - categorical_accuracy: 0.6469 - val_loss: 0.4087 - val_categorical_accuracy: 0.8596\nEpoch 3/15\n267/267 [==============================] - 87s 324ms/step - loss: 0.7852 - categorical_accuracy: 0.7458 - val_loss: 0.4027 - val_categorical_accuracy: 0.8596\nEpoch 4/15\n267/267 [==============================] - 85s 319ms/step - loss: 0.7599 - categorical_accuracy: 0.7588 - val_loss: 0.5051 - val_categorical_accuracy: 0.8338\nEpoch 5/15\n267/267 [==============================] - 87s 326ms/step - loss: 0.7301 - categorical_accuracy: 0.7741 - val_loss: 0.4132 - val_categorical_accuracy: 0.8636\nEpoch 6/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.7092 - categorical_accuracy: 0.7826 - val_loss: 0.4415 - val_categorical_accuracy: 0.8577\nEpoch 7/15\n267/267 [==============================] - 84s 315ms/step - loss: 0.6805 - categorical_accuracy: 0.7901 - val_loss: 0.4087 - val_categorical_accuracy: 0.8693\nEpoch 8/15\n267/267 [==============================] - 82s 309ms/step - loss: 0.6611 - categorical_accuracy: 0.8016 - val_loss: 0.3696 - val_categorical_accuracy: 0.8755\nEpoch 9/15\n267/267 [==============================] - 83s 311ms/step - loss: 0.6366 - categorical_accuracy: 0.8019 - val_loss: 0.3636 - val_categorical_accuracy: 0.8821\nEpoch 10/15\n267/267 [==============================] - 85s 319ms/step - loss: 0.6196 - categorical_accuracy: 0.8193 - val_loss: 0.3565 - val_categorical_accuracy: 0.8875\nEpoch 11/15\n267/267 [==============================] - 82s 307ms/step - loss: 0.6060 - categorical_accuracy: 0.8154 - val_loss: 0.3848 - val_categorical_accuracy: 0.8693\nEpoch 12/15\n267/267 [==============================] - 83s 311ms/step - loss: 0.6246 - categorical_accuracy: 0.8112 - val_loss: 0.3782 - val_categorical_accuracy: 0.8679\nEpoch 13/15\n267/267 [==============================] - 84s 316ms/step - loss: 0.5956 - categorical_accuracy: 0.8193 - val_loss: 0.3385 - val_categorical_accuracy: 0.8897\nEpoch 14/15\n267/267 [==============================] - 82s 309ms/step - loss: 0.5878 - categorical_accuracy: 0.8221 - val_loss: 0.3439 - val_categorical_accuracy: 0.8878\nEpoch 15/15\n267/267 [==============================] - 86s 323ms/step - loss: 0.5688 - categorical_accuracy: 0.8387 - val_loss: 0.3178 - val_categorical_accuracy: 0.8970\nFOLD-3\n\nEpoch 1/15\n267/267 [==============================] - 203s 400ms/step - loss: 1.6043 - categorical_accuracy: 0.2268 - val_loss: 1.5857 - val_categorical_accuracy: 0.2346\nEpoch 2/15\n267/267 [==============================] - 82s 309ms/step - loss: 1.0046 - categorical_accuracy: 0.6681 - val_loss: 0.4476 - val_categorical_accuracy: 0.8419\nEpoch 3/15\n267/267 [==============================] - 87s 325ms/step - loss: 0.7733 - categorical_accuracy: 0.7513 - val_loss: 0.3968 - val_categorical_accuracy: 0.8677\nEpoch 4/15\n267/267 [==============================] - 86s 321ms/step - loss: 0.7375 - categorical_accuracy: 0.7702 - val_loss: 0.5392 - val_categorical_accuracy: 0.8030\nEpoch 5/15\n267/267 [==============================] - 87s 327ms/step - loss: 0.7236 - categorical_accuracy: 0.7767 - val_loss: 0.4228 - val_categorical_accuracy: 0.8591\nEpoch 6/15\n267/267 [==============================] - 83s 310ms/step - loss: 0.7223 - categorical_accuracy: 0.7764 - val_loss: 0.4171 - val_categorical_accuracy: 0.8646\nEpoch 7/15\n267/267 [==============================] - 83s 311ms/step - loss: 0.7028 - categorical_accuracy: 0.7852 - val_loss: 0.5702 - val_categorical_accuracy: 0.7933\nEpoch 8/15\n267/267 [==============================] - 84s 315ms/step - loss: 0.6686 - categorical_accuracy: 0.8015 - val_loss: 0.4437 - val_categorical_accuracy: 0.8480\nEpoch 9/15\n267/267 [==============================] - 81s 305ms/step - loss: 0.6486 - categorical_accuracy: 0.8044 - val_loss: 0.4712 - val_categorical_accuracy: 0.8402\nEpoch 10/15\n267/267 [==============================] - 84s 314ms/step - loss: 0.6374 - categorical_accuracy: 0.8146 - val_loss: 0.3694 - val_categorical_accuracy: 0.8769\nEpoch 11/15\n267/267 [==============================] - 81s 305ms/step - loss: 0.6051 - categorical_accuracy: 0.8133 - val_loss: 0.3558 - val_categorical_accuracy: 0.8812\nEpoch 12/15\n267/267 [==============================] - 81s 303ms/step - loss: 0.6139 - categorical_accuracy: 0.8177 - val_loss: 0.3569 - val_categorical_accuracy: 0.8767\nEpoch 13/15\n267/267 [==============================] - 83s 309ms/step - loss: 0.5954 - categorical_accuracy: 0.8187 - val_loss: 0.3396 - val_categorical_accuracy: 0.8793\nEpoch 14/15\n267/267 [==============================] - 78s 292ms/step - loss: 0.5754 - categorical_accuracy: 0.8288 - val_loss: 0.3616 - val_categorical_accuracy: 0.8809\nEpoch 15/15\n267/267 [==============================] - 78s 292ms/step - loss: 0.5859 - categorical_accuracy: 0.8251 - val_loss: 0.3342 - val_categorical_accuracy: 0.8838\nFOLD-4\n\nEpoch 1/15\n267/267 [==============================] - 201s 392ms/step - loss: 1.5685 - categorical_accuracy: 0.2856 - val_loss: 1.5776 - val_categorical_accuracy: 0.2959\nEpoch 2/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.9812 - categorical_accuracy: 0.6718 - val_loss: 0.4136 - val_categorical_accuracy: 0.8603\nEpoch 3/15\n267/267 [==============================] - 86s 321ms/step - loss: 0.7825 - categorical_accuracy: 0.7509 - val_loss: 0.5642 - val_categorical_accuracy: 0.7879\nEpoch 4/15\n","name":"stdout"},{"output_type":"stream","text":"267/267 [==============================] - 80s 301ms/step - loss: 0.7381 - categorical_accuracy: 0.7700 - val_loss: 0.5004 - val_categorical_accuracy: 0.8456\nEpoch 5/15\n267/267 [==============================] - 81s 305ms/step - loss: 0.7363 - categorical_accuracy: 0.7738 - val_loss: 0.4635 - val_categorical_accuracy: 0.8378\nEpoch 6/15\n267/267 [==============================] - 84s 314ms/step - loss: 0.7231 - categorical_accuracy: 0.7713 - val_loss: 0.4066 - val_categorical_accuracy: 0.8679\nEpoch 7/15\n267/267 [==============================] - 82s 306ms/step - loss: 0.6846 - categorical_accuracy: 0.7912 - val_loss: 0.4107 - val_categorical_accuracy: 0.8658\nEpoch 8/15\n267/267 [==============================] - 86s 324ms/step - loss: 0.6567 - categorical_accuracy: 0.7972 - val_loss: 0.4786 - val_categorical_accuracy: 0.8390\nEpoch 9/15\n267/267 [==============================] - 81s 303ms/step - loss: 0.6497 - categorical_accuracy: 0.8030 - val_loss: 0.3750 - val_categorical_accuracy: 0.8759\nEpoch 10/15\n267/267 [==============================] - 83s 310ms/step - loss: 0.6371 - categorical_accuracy: 0.8092 - val_loss: 0.3479 - val_categorical_accuracy: 0.8838\nEpoch 11/15\n267/267 [==============================] - 85s 317ms/step - loss: 0.6273 - categorical_accuracy: 0.8080 - val_loss: 0.3655 - val_categorical_accuracy: 0.8804\nEpoch 12/15\n267/267 [==============================] - 77s 290ms/step - loss: 0.6179 - categorical_accuracy: 0.8142 - val_loss: 0.3489 - val_categorical_accuracy: 0.8840\nEpoch 13/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.6001 - categorical_accuracy: 0.8161 - val_loss: 0.3245 - val_categorical_accuracy: 0.8918\nEpoch 14/15\n267/267 [==============================] - 82s 308ms/step - loss: 0.5892 - categorical_accuracy: 0.8276 - val_loss: 0.3476 - val_categorical_accuracy: 0.8816\nEpoch 15/15\n267/267 [==============================] - 78s 291ms/step - loss: 0.5644 - categorical_accuracy: 0.8369 - val_loss: 0.3275 - val_categorical_accuracy: 0.8928\nFOLD-5\n\nEpoch 1/15\n267/267 [==============================] - 198s 385ms/step - loss: 1.5698 - categorical_accuracy: 0.2781 - val_loss: 1.6326 - val_categorical_accuracy: 0.1745\nEpoch 2/15\n267/267 [==============================] - 79s 297ms/step - loss: 0.9802 - categorical_accuracy: 0.6673 - val_loss: 0.4281 - val_categorical_accuracy: 0.8438\nEpoch 3/15\n267/267 [==============================] - 84s 314ms/step - loss: 0.7675 - categorical_accuracy: 0.7610 - val_loss: 0.4529 - val_categorical_accuracy: 0.8558\nEpoch 4/15\n267/267 [==============================] - 80s 298ms/step - loss: 0.7345 - categorical_accuracy: 0.7698 - val_loss: 0.5149 - val_categorical_accuracy: 0.8210\nEpoch 5/15\n267/267 [==============================] - 81s 303ms/step - loss: 0.7143 - categorical_accuracy: 0.7751 - val_loss: 0.4773 - val_categorical_accuracy: 0.8416\nEpoch 6/15\n267/267 [==============================] - 84s 313ms/step - loss: 0.7229 - categorical_accuracy: 0.7786 - val_loss: 0.4431 - val_categorical_accuracy: 0.8577\nEpoch 7/15\n267/267 [==============================] - 79s 294ms/step - loss: 0.6916 - categorical_accuracy: 0.7842 - val_loss: 0.4467 - val_categorical_accuracy: 0.8464\nEpoch 8/15\n267/267 [==============================] - 80s 301ms/step - loss: 0.6617 - categorical_accuracy: 0.8025 - val_loss: 0.4706 - val_categorical_accuracy: 0.8497\nEpoch 9/15\n267/267 [==============================] - 81s 304ms/step - loss: 0.6404 - categorical_accuracy: 0.8077 - val_loss: 0.3957 - val_categorical_accuracy: 0.8752\nEpoch 10/15\n267/267 [==============================] - 77s 288ms/step - loss: 0.6244 - categorical_accuracy: 0.8112 - val_loss: 0.3877 - val_categorical_accuracy: 0.8703\nEpoch 11/15\n267/267 [==============================] - 78s 291ms/step - loss: 0.6008 - categorical_accuracy: 0.8177 - val_loss: 0.3575 - val_categorical_accuracy: 0.8804\nEpoch 12/15\n267/267 [==============================] - 81s 303ms/step - loss: 0.6107 - categorical_accuracy: 0.8158 - val_loss: 0.3671 - val_categorical_accuracy: 0.8724\nEpoch 13/15\n267/267 [==============================] - 76s 287ms/step - loss: 0.5901 - categorical_accuracy: 0.8234 - val_loss: 0.3463 - val_categorical_accuracy: 0.8835\nEpoch 14/15\n267/267 [==============================] - 78s 293ms/step - loss: 0.5869 - categorical_accuracy: 0.8225 - val_loss: 0.3384 - val_categorical_accuracy: 0.8875\nEpoch 15/15\n267/267 [==============================] - 80s 300ms/step - loss: 0.5934 - categorical_accuracy: 0.8215 - val_loss: 0.3406 - val_categorical_accuracy: 0.8828\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}